{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pylab\n",
    "import imageio\n",
    "filename = 'arson.mp4'\n",
    "vid = imageio.get_reader(filename,  'ffmpeg')\n",
    "frames=[]\n",
    "\n",
    "for i, im in enumerate(vid):\n",
    "    frames.append(im)\n",
    "frames = torch.from_numpy(np.array(frames, dtype ='float'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4187, 240, 320, 3])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frames.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "nums = [2000, 2100]\n",
    "for num in nums:\n",
    "    image = vid.get_data(num)\n",
    "    fig = pylab.figure()\n",
    "    fig.suptitle('image #{}'.format(num), fontsize=20)\n",
    "    pylab.imshow(image)\n",
    "pylab.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "pylab.imshow(frames[1000]/255)\n",
    "pylab.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(66.13205, dtype=torch.float64)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frames[1000].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\derek/.cache\\torch\\hub\\ultralytics_yolov5_master\n",
      "YOLOv5  2023-5-2 Python-3.10.10 torch-1.13.1 CUDA:0 (NVIDIA GeForce RTX 3080, 10240MiB)\n",
      "\n",
      "Fusing layers... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m\u001b[1mrequirements:\u001b[0m C:\\Users\\derek\\.cache\\torch\\hub\\requirements.txt not found, check failed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients\n",
      "Adding AutoShape... \n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "yolo = torch.hub.load(\"ultralytics/yolov5\", \"yolov5s\", pretrained=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 240, 320, 3])\n"
     ]
    }
   ],
   "source": [
    "frames = torch.from_numpy(np.array(frames, dtype='float32'))\n",
    "frames_100 = frames[2000:2100,:,:,:]\n",
    "print(frames_100.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(77.50818)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frames_100[99].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.is_available()\n",
    "frames = frames.cpu()\n",
    "print(frames.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "510.0\n"
     ]
    }
   ],
   "source": [
    "input = [its.cpu().detach().numpy() * 255 for its in frames]\n",
    "print(input[0].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcome = []\n",
    "frames_100 = torch.permute(frames_100, (0, 2, 3, 1))\n",
    "predictions = yolo([its.cpu().detach().numpy() for its in frames_100])\n",
    "\n",
    "for i in range(frames_100.shape[0]):\n",
    "        single_result = predictions.pandas().xyxy[i]\n",
    "        pred_class = single_result[\"name\"].tolist()\n",
    "        pred_score = single_result[\"confidence\"].tolist()\n",
    "        pred_boxes = single_result[[\"xmin\", \"ymin\", \"xmax\", \"ymax\"]].apply(\n",
    "            lambda x: list(x), axis=1\n",
    "        )\n",
    "\n",
    "        outcome.append(\n",
    "                {\n",
    "                    \"labels\": pred_class,\n",
    "                    \"bboxes\": pred_boxes,\n",
    "                    \"scores\": pred_score,\n",
    "                },\n",
    "            )\n",
    "result = pd.DataFrame(\n",
    "            outcome,\n",
    "            columns=[\n",
    "                \"labels\",\n",
    "                \"bboxes\",\n",
    "                \"scores\",\n",
    "            ],\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     [car, person, skateboard, car, car]\n",
       "1                 [car, person, car, car]\n",
       "2                         [car, car, car]\n",
       "3                 [car, person, car, car]\n",
       "4                         [car, car, car]\n",
       "                     ...                 \n",
       "95                                  [car]\n",
       "96                                  [car]\n",
       "97                                  [car]\n",
       "98                                  [car]\n",
       "99                                  [car]\n",
       "Name: labels, Length: 100, dtype: object"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[\"labels\"]\n",
    "[3, 1, 1, ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     0    [167.02383422851562, 82.59362030029297, 1...\n",
       "1     0    [167.0209197998047, 82.59158325195312, 18...\n",
       "2     0    [167.02590942382812, 82.59093475341797, 1...\n",
       "3     0    [167.00804138183594, 82.5923080444336, 18...\n",
       "4     0    [167.0108642578125, 82.57781982421875, 18...\n",
       "                            ...                        \n",
       "95    0    [100.26240539550781, 99.43904113769531, 1...\n",
       "96    0    [100.2587661743164, 99.4135513305664, 135...\n",
       "97    0    [100.26038360595703, 99.42559814453125, 1...\n",
       "98    0    [100.27104187011719, 99.4421615600586, 13...\n",
       "99    0    [100.25949096679688, 99.44524383544922, 1...\n",
       "Name: bboxes, Length: 100, dtype: object"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[\"bboxes\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     [0.6064822673797607, 0.5089840292930603, 0.265...\n",
       "1     [0.6099070906639099, 0.2898620367050171, 0.260...\n",
       "2     [0.6088065505027771, 0.2546219825744629, 0.250...\n",
       "3     [0.6069881916046143, 0.32793930172920227, 0.26...\n",
       "4     [0.6033501029014587, 0.26143956184387207, 0.26...\n",
       "                            ...                        \n",
       "95                                 [0.5185160040855408]\n",
       "96                                 [0.5155003666877747]\n",
       "97                                  [0.511539876461029]\n",
       "98                                 [0.5100327730178833]\n",
       "99                                 [0.5057608485221863]\n",
       "Name: scores, Length: 100, dtype: object"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[\"scores\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from pprint import pprint\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def annotate_video(detections, input_video_path, output_video_path):\n",
    "    color1=(207, 248, 64)\n",
    "    color2=(255, 49, 49)\n",
    "    thickness=4\n",
    "\n",
    "    vcap = cv2.VideoCapture(input_video_path)\n",
    "    width = int(vcap.get(3))\n",
    "    height = int(vcap.get(4))\n",
    "    fps = vcap.get(5)\n",
    "    fourcc = cv2.VideoWriter_fourcc('m', 'p', '4', 'v') #codec\n",
    "    video=cv2.VideoWriter(output_video_path, fourcc, fps, (width,height))\n",
    "\n",
    "    frame_id = 0\n",
    "    # Capture frame-by-frame\n",
    "    # ret = 1 if the video is captured; frame is the image\n",
    "    ret, frame = vcap.read() \n",
    "\n",
    "    while ret:\n",
    "        df = detections\n",
    "        df = df[['bboxes', 'labels']][df.index == frame_id]\n",
    "        if df.size:\n",
    "            dfLst = df.values.tolist()\n",
    "            for bbox, label in zip(dfLst[0][0], dfLst[0][1]):\n",
    "                x1, y1, x2, y2 = bbox\n",
    "                x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)\n",
    "                # object bbox\n",
    "                frame=cv2.rectangle(frame, (x1, y1), (x2, y2), color1, thickness) \n",
    "                # object label\n",
    "                cv2.putText(frame, label, (x1, y1-10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, color1, thickness) \n",
    "                # frame label\n",
    "                cv2.putText(frame, 'Frame ID: ' + str(frame_id), (700, 500), cv2.FONT_HERSHEY_SIMPLEX, 1.2, color2, thickness) \n",
    "            video.write(frame)\n",
    "\n",
    "            # Stop after twenty frames (id < 20 in previous query)\n",
    "            if frame_id == 4000:\n",
    "                break\n",
    "\n",
    "            # Show every fifth frame\n",
    "            if frame_id % 5 == 0:\n",
    "                plt.imshow(frame)\n",
    "                plt.show()\n",
    "\n",
    "        \n",
    "        frame_id+=1\n",
    "        ret, frame = vcap.read()\n",
    "\n",
    "    video.release()\n",
    "    vcap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38fb0040743f44f1838b1621eb1e88e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Video(value=b'\\x00\\x00\\x00\\x1cftypisom\\x00\\x00\\x02\\x00isomiso2mp41\\x00\\x00\\x00\\x08free\\x00\\x01\\xbd\\x18...')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ipywidgets import Video, Image\n",
    "input_path = 'arson.mp4'\n",
    "output_path = 'output.mp4'\n",
    "annotate_video(result, input_path, output_path)\n",
    "Video.from_file(output_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
